import os
import joblib 
from src.preprocess import prepare_datasets
from src.model import build_mtslstm_model
from src.utils import create_scalers
from tensorflow.keras.callbacks import EarlyStopping


def train_model(config):
    # Create scaler dict
    scalers = create_scalers()

    # Táº¡o dá»¯ liá»‡u train/test tá»« file + scaler
    X1D_train, X1h_train, y_train, X1D_test, X1h_test, y_test = prepare_datasets(config, scalers)

    input_shapes = {
        '1D': (X1D_train.shape[1], X1D_train.shape[2]),
        '1h': (X1h_train.shape[1], X1h_train.shape[2])
    }

    # Build model
    model = build_mtslstm_model(config, input_shapes)

    # Training
    print(model.summary())
    model.fit(
        [X1D_train, X1h_train], y_train,
        validation_data=([X1D_test, X1h_test], y_test),
        epochs=config['training']['epochs'],
        batch_size=config['training']['batch_size'],
        callbacks=[EarlyStopping(patience=10, restore_best_weights=True)],
        verbose=1
    )

    print("âœ… Huáº¥n luyá»‡n hoÃ n táº¥t.")

     # ğŸ“ Táº¡o thÆ° má»¥c náº¿u chÆ°a tá»“n táº¡i
    os.makedirs("trained-models", exist_ok=True)

    # ğŸ’¾ LÆ°u model
    model.save("trained-models/mtslstm_model.keras")  # dÃ¹ng Ä‘á»‹nh dáº¡ng má»›i
    print("ğŸ’¾ ÄÃ£ lÆ°u model táº¡i: trained-models/mtslstm_model.keras")

    # ğŸ’¾ LÆ°u scalers (thÃªm vÃ o Ä‘Ã¢y)
    joblib.dump(scalers['1D'], "trained-models/scaler_1D.pkl")
    joblib.dump(scalers['1h'], "trained-models/scaler_1h.pkl")
    print("ğŸ’¾ ÄÃ£ lÆ°u scaler vÃ o thÆ° má»¥c trained-models")

    return model, scalers



